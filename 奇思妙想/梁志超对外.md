## 目标
本设计智能语音机器人系统总体框图，主要包括物体识别、单目测距，语音交互、运动控制四大模块，如图2所示。综合软硬件设计，机器人实现了精准智能的语音播报,以提示盲人前方物体的距离及方向，使盲人能够准确了解前方环境。
代办：
引言：视觉是人类获取知识和经验的主要感知来源，约占人体总媒介信息来源的80%，丧失视觉能力，意味着人们将失去最重要的信息与知识的感知来源。当今生活环境日益复杂，这给盲人的生活带来了更多的不便，甚至面临危险。视力有障碍的人都希望有一个辅助自己来到达目的地的机器人，让他们像正常人一样行走，或者说能保证让他们在行走过程中感到安全。给盲人提供一个安全、可靠、智能、高效的出行方式，是彰显人人平等、时代快速发展重要标志。“十四五”规划和2035 年远景目标明确指出要健全多层次社会保障体系。智能语音机器人不仅能够在导盲辅助设备研究中发挥优势，也能在提高低视力人群生活质量的同时，为视觉障碍患者提供多种帮助。
### 原理
1.智能语音导盲的方案设计
针对盲人视觉上的缺陷可以知道，盲人生活的周围环境不会过于复杂，我们需要利用多个传感器融合技术以及目前比较热门的机器视觉相关算法来探测盲人前方的障碍物，进而大大方便盲人的出行。
在目标检测与识别方面，采用基于 Darknet网络结构的You Only Look Once(YOLO)算法对待测物体的图像库进行训练和识别测试，从而实现对图像中多个目标的检测与识别。控制单元系统主要结构包括了一个主控单元、检测单元、运动控制处理单元，语音控制处理单元四大部分。主控单元主要功能是专门用来设计实现对各运动模块之间的运动协调控制，利用超声波检测与障碍物的距离。运动控制单元主要负责对机器人运动状态进行实时运动控制，其中主要包括控制机器人左转、右转、直行。语音控制单元主要是通过语音识别模块进行识别。硬件方面，优化避障效果，配备了单目摄像机、WIFI设备，完成信息采集和通信功能。该设计方案由硬件部分和软件部分构成，如图1所示。![1](https://www.helloimg.com/i/2024/10/25/671b5a9b50e6e.png)

2 目标检测算法研究
2.1物体检测与识别
机器人对周围环境的感知，精准地对物体进行检测与识别是本系统的重点，故要对于摄像机接收到的图像信号中多物体进行检测并精准识别出物体类别。目标检测的方法有很多种，常见的算法有 Region-CNN(R-CNN), Single Shot MultiBox De- tector(SSD)以及 YOLO, R-CNN模型不能快速的定位物体，在单一图片处理上比较浪费时间； SSD模型会随着输入图片的规格扩大，从而导致效率下降：YOLO算法的出现无疑是在目标检测的领域内带来了极大的便利，该算法进行合理变换可以保证实时性的基础上给特定物体的识别效果带来很大的提升[1]。
2.2 YOLO算法
文献[2]中提到，YOLO 是一种实时的且准确的对象检测算法，YOLO V1版本已可以检测每秒四十多张图片，随着版本的更新换代，YOLO算法已经更新了4代，其中还伴随更新了一些轻量级算法，文中选用的是目前兼具速度和检测质量的YO-LO V3版本，YOLO算法是属于卷积神经网络，它是由卷积层、全连接层还有池化层组成的。其训练的样本无需从样本图像中特意裁剪出，而是对整个图像进行训练和检测，提升了系统的稳定性。需要检测的图像被分割为n×n个网格，每个格子都分别负责检测是否有被测物体的中心落在了相应的格子内，当每一个小物体的网格需要检测时，自动产生一个被检测者所看到的中心框，每个被检测的中心边框都包括5维的物体信息(x,y,w,h,Cobject),x.代表的是边框中心的横坐标，y代表的是纵坐标，w和h分别代表的整个被测照片的宽度、高度，Cobject代表的是包围框的置信度，置信度如公式(1)所示：
Cobject=Pr( Object)*IOUtruth pred (1)
式(1)中,Pr(Object)为物体在落在相应格子里的几率,若存在,该值是1,不存在就是0,IOUtruth pred表示交并比(参考的标准框和检测框的交并面积之比)。
YOLO算法在卷积层[3]中提取相应特征，在全连接层进行目标预测功能。当 Pr(Object)的值为1的时候，这幅图片的置信度如式(2)所示。
Pr(Class|Object)* Pr(Object)*IOUtruth pred=Pr(Class)*IOUtruth pred  (2)
式(2)中 Pr(Class|Object)是待检测目标的分类条件概率。Pr(Class)是预测了某类别的概率。经过上述计算后，设置被检测包围边框的阈值，滤掉得分低于阈值的包围边框，并对被检测包围边框中所有阈值以下的包围边框，进行非极大阈值的抑制处理，即可获得被检测的结果。
2.3 YOLO V3 网络
文中物体检测部分采用 YOLO V3。YOLOV3 是在 2018年提出来的,该检测系统基于 Dark- net-53[4]深度学习框架环境进行物体识别，融合 Feature Pyramid Networks(FPN)思想,预测三种尺度的框，解决了小目标检测算法上的效果不好的问题。YOLO V3不仅可以实现图片中的目标检测，还可以实现对运动目标的实时检测。Darknet53的网络结构共包含了53个卷积层。
5个残差块构成了 Darknet53 的网络结构，残差神经网络[5]的思想在Darknet53的网络结构里有了充分的应用。众多的残差单元构成一个个的残差块，通过残差单元输入两个数码累计DBL来执行操作，就这样构成了残差单元，其中DBL 单元包含了卷积层归一化和 leaky ReLU激活函数。YOLO V3对检测的图片进行了5次降采样[6]，最后3次降采样中对目标进行预测。YOLO V3算法主要由训练模块、模型模块、预测模块、检测模块组成，其相关联系如图3。其中模型模块是 YOLO V3的核心，其作用就是根据给定的网络结构构建对应的模型，模型用于训练与检测。
![2](https://www.helloimg.com/i/2024/10/25/671b5b00af0a7.png)
3 基于单目视觉的目标测距
3.1  相机标定
当今世界，准确测量物体的空间位置是计算机视觉研究中的重点问题。测量的方法分为单目标定、双目标定、多目标定。单目和双目算法比较成熟，使用较多。针对两者进行比较，单目成本低，操作简单，但测距精度不高；双目操作复杂，测距精度高。由于本设计对于测距精度的要求不高，综合考虑成本和实现可能性，该机器人采用单目标定及测距完成机器人对最近物体距离的感知。
以被测物体的几何中心为原点，建立如图4所示的世界坐标系[7]，选择物体上除了原点之外的另外四个点并得到坐标，以便后续的计算。
![3](https://www.helloimg.com/i/2024/10/25/671b5b3657974.png)

下面对三种空间下的坐标系进行了简要说明：
世界坐标系：系统的绝对坐标系，在没有建立用户坐标系之前画面上所有点的坐标都是以该坐标系的原点来确定各自的位置。
摄像机坐标系：以相机的光心为坐标原点，X轴和Y轴分别平行于图像坐标系的X 轴和Y轴，相机的光轴为Z轴，用(XCYCZC)来表示坐标，
图像坐标系：图像的左上角作为原点做为基准建立的坐标系，来描述图像中被测物体的位置，如式(3):
![4](https://www.helloimg.com/i/2024/10/25/671b5b5b568ad.png)

式(3)中，t是3×3的平移矩阵，R是单位正交矩阵。
为了简化处理，将其转为平面坐标系，当图像成像坐标系中任意一点(x，y)在平面坐标系对应点为(m,n),则有式(4)的转化关系。

![5](https://www.helloimg.com/i/2024/10/25/671b5b9273006.png)

其中，dx表示传感器单个像素的宽，dy表示传感器单个像素的高，s0代表摄像机坐标系梁坐标轴之间的倾斜因子，m0和n0表示实际成像的坐标。
摄像机坐标系中任意一点( XC，YC)在成像平面上对应的坐标为(x，y)，则可以得到式(5)

![6](https://www.helloimg.com/i/2024/10/25/671b65778a04c.png)

根据上述所描述的三个坐标系的转换关系，利用单目摄像头内部参数，可以大致的检测到距离前面障碍物的距离。根据关于成像模型中透视几何关系的推导[8],可以估算出于障碍物的距离。
将摄像头与主控制器设备连接，操作系统选用的 win10，编写驱动程序，图像处理库采用 mat-lab2016进行标定，标定过程大致分为六步：第一步，采集照片让棋盘在照片中占据最大比例，这样可以得到更多的信息，同时固定好相机，第二通过多个角度拍摄，本次拍摄了10张图片；第三步进行 Matlab camera calibration标定工具箱进行标定;第四步导入预先拍好的照片，采用25mm的单元格；第五步设置校正参数为-0.2;第六步点击 Calibrate开始标定，并导出计算结果，完成单目标定。整体过程如图5所示。

![7](https://www.helloimg.com/i/2024/10/25/671b659083bc0.png)

3.2 单目校正
校正过程中，经常会出现畸变现象，我们通常使用的相机焦距很小，可视为凸透镜。当光通过不同厚度的透镜时，光的折叠方式会不同，这称为径向畸变，其远离透镜中心的光线比靠近透镜中心的光线弯曲得更严重，不利于我们计算图像物体的高度。除了径向畸变外，还有一个非常重要的切向畸变，由于镜头本身与像面不平行所导致的。
在MATLAB中，校正比较方便，只需要点击 Calibrate 开始标定后,点击 Show Undistorted 即可查看校正后的图像,然后再点击 Export Camera Parameters 即可保存参数。
3.3  单目测距
单目测距一般有两种方式，第一种是根据定位测量插值得到每个像素的坐标，第二种是根据相似三角比例计算出对应像素点的坐标。第一种必须固定摄像头，轻微的移动都会导致测量坐标的误差增大，本文利用第二种方式，利用相似三角形的几何关系，也是利用了小孔成像原理，获取图像的深度信息。

![8](https://www.helloimg.com/i/2024/10/25/671b65a67a23c.png)

如图6所示，H为摄像头光心，P是世界坐标系中一点，Px是P点在世界坐标系Xw轴上的投影，Py是P点在世界坐标系Yw上的投影。α是固定相机的俯仰角，h是实际测量获取的摄像头光心Oc距离地面的距离。假设P点与摄像头的水平距离为Dy，垂直方向距离 Dₓ，则有
得到待测物体与成像平面的水平距离，即物体深度。
4 实验分析
4.1  硬件系统实验结果分析
本文以 Arduino为主控板，结合多种传感器来完成机器人的运动。本设计可以控制机器人在运动中如何进行前进、后退、左转、右转、停止。还能实现自主避障的功能。在设计中，运用了超声波笼机云平台、避障传感器进行避障，用来检测机器人周围的障碍物，运用语音识别模块进行盲人与机器人的交流。并结合 WIFI模块进行通信。初步设计是障碍物距离机器人10 cm时候超声波模块传给 Arduino，然后避障传感器进行避障，并通过语音形式告诉盲人障碍物的位置。
4.2  软件系统实验结果分析
基于YOLO V3的物体检测与识别，可以对一张图片中多个物体进行检测，并可以将物体的类别
根据单目视觉的测距系统，对不同距离进行了各20次测距实验。如表1所示，对实际距离、测量距离、相对误差进行分析。

|实际距离|单次估算距离|20次平均|
|----|------|----|
|20|20.5|5%|
|30|30.6|3%|
|40|39.8|1%|
|50|58.2|5%|
|60|62.6|6%|

如表1所示，其中，实际距离是指相机距离待测物体的实际距离；单次估算距离指在20次测距实验中随机挑选1次以示例显示本设计单目测距的情况，根据实际测量可知，单目系统测距值会在实际值上下浮动；20次平均误差是每次相对误差的平均值，即单目测距系统平均相对误差。
分析不同距离时的测距结果得知，单目视觉测距误差稍大[9]，但符合本系统的导盲要求，且成本较低。
结合 Yolo识别结果和单目测距的结果，系统将自动进行语音播报，举例说明语音播报内容：前方发现障碍物凳子，距离您40厘米，请注意。
参 考 文 献
[1] 云鹏，侯凌燕，王超.基于 YOLO V3 的自动驾驶中运动目标检测[J].计算机工程与技术,2019,40(4);246-251.
[2] Redmon J, Divvala S, Girshick R, et al. You Only Look Once: Unified, Real- Time Object Detection[C]// Computer Vision and Pattern Recognition.IEEE,2016:779-788.
[3] 张富凯，杨峰，李策.基于改进 YOLOV3的快速车辆检测方法[J].计算机工程与应用,2019,55(2):1220.
[4] 刘博,王胜正,赵建森等.基于 darknet 网络和 YOLOV3算法的船舶跟踪识别[J].计算机应用，2019，39(6):1663-1668.
[5] He K M, Zhang X Y, Ren S Q, et al, Deep residual learning for image recognition [C] //2016 IEEE Con- ference on Computer Vision and Pattern Recongnition(CVPR),Jnue27-30,2016, Lasvegas,NV,USA. NEW York:IEEE,2016:770 778.
[6] 鞠默然,罗海波,王仲博,等.改进的 YOLO V3 算法[J].光学学报,2019,34(10);2-3.
[7] 胡兆伦.基于智能手机的车辆检测与车距测量[D].浙江:浙江大学,2017.
[8] 王文波.基于单日视觉的实时测距方法研究[D].大连理工大学,2014.
[9] 王政博，唐勇，刘海波，孙东来，栗梦媛.基于视觉伺服的 Baxter机器人手臂跟随系统研究[J].河北水利电力学院学报,2021,31(2):61-66.
